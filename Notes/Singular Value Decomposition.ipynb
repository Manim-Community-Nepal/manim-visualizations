{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f18fcf",
   "metadata": {},
   "source": [
    "**Generated By Google Gemini:**\n",
    "\n",
    "# ðŸ“– What is Singular Value Decomposition (SVD)?\n",
    "\n",
    "\n",
    "\n",
    "In simple terms, **Singular Value Decomposition (SVD)** is a method of breaking down *any* matrix into a product of three simpler, more fundamental matrices.\n",
    "\n",
    "Think of a matrix $A$ as a \"transformation machine.\" You put a vector (a point in space) $\\vec{x}$ into it, and it gives you a new vector $\\vec{y}$ out: $A\\vec{x} = \\vec{y}$. This transformation can be complex: it might stretch, squash, rotate, and shear all at once.\n",
    "\n",
    "SVD tells us that this complex machine, $A$, is *secretly* just a simple three-step process:\n",
    "\n",
    "1.  **Step 1: A Rotation** (or reflection).\n",
    "2.  **Step 2: A Scaling** (stretching or squashing along the new rotated axes).\n",
    "3.  **Step 3: Another Rotation** (or reflection).\n",
    "\n",
    "The SVD \"recipe\" is written as:\n",
    "$$\n",
    "A = U \\Sigma V^T\n",
    "$$\n",
    "\n",
    "* **$A$** is your original $m \\times n$ matrix.\n",
    "* **$U$** is the $m \\times m$ matrix for the final rotation.\n",
    "* **$\\Sigma$** (Sigma) is the $m \\times n$ matrix for the scaling.\n",
    "* **$V^T$** is the $n \\times n$ matrix for the initial rotation.\n",
    "\n",
    "To truly understand this, we must first understand the \"ingredients\": $U$, $\\Sigma$, and $V^T$.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¥£ Part 1: The \"Ingredients\" Explained\n",
    "\n",
    "Let's define the simple concepts that SVD is built from.\n",
    "\n",
    "#### Matrix\n",
    "A matrix is a grid of numbers that acts as a transformation machine.\n",
    "* **Example:** $A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$.\n",
    "    If you put the vector $\\vec{x} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ (a point at x=1, y=0) into it, you get:\n",
    "    $A\\vec{x} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\n",
    "    The matrix $A$ moved the point $(1, 0)$ to $(2, 1)$. This is a stretch and a shear.\n",
    "\n",
    "#### Matrix Transpose ($A^T$)\n",
    "The transpose \"flips\" a matrix along its main diagonal. Rows become columns, and columns become rows.\n",
    "* **Example:** If $A = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$, then $A^T = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}$. (This matrix is *symmetric*).\n",
    "* **Example 2:** If $B = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$, then $B^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}$.\n",
    "\n",
    "#### Diagonal Matrix\n",
    "A matrix where all entries are zero *except* for the values on the main diagonal. Its only job is to **scale** (stretch or squash) space along the standard axes (the x-axis, y-axis, etc.).\n",
    "* **Example:** $D = \\begin{bmatrix} 3 & 0 \\\\ 0 & 0.5 \\end{bmatrix}$.\n",
    "    This matrix transforms a vector $\\begin{bmatrix} x \\\\ y \\end{bmatrix}$ into $\\begin{bmatrix} 3x \\\\ 0.5y \\end{bmatrix}$. It stretches space by 3x along the x-axis and squashes it by 0.5x along the y-axis.\n",
    "\n",
    "#### Orthogonal Matrix ($Q$)\n",
    "This is the most important ingredient. It represents a **pure rotation or reflection**.\n",
    "* **Geometric Property:** It **preserves lengths and angles**.\n",
    "    * If you take a vector $\\vec{x}$, its length *does not change* when you transform it with $Q$.\n",
    "    * If you take two perpendicular vectors, $\\vec{x}$ and $\\vec{y}$, their transformed versions ($Q\\vec{x}$ and $Q\\vec{y}$) will *still be perpendicular*.\n",
    "* **Mathematical Property:** Its columns are **orthonormal**.\n",
    "    * *Ortho-* (Orthogonal): Each column vector is perpendicular (at a 90Â° angle) to every other column vector.\n",
    "    * *-normal* (Normalized): Each column vector has a length of 1.\n",
    "* **The Key Test:** A matrix $Q$ is orthogonal if its transpose is also its inverse.\n",
    "    > $Q^T Q = I$ (the identity matrix)\n",
    "    This means \"undoing\" the rotation ($Q^{-1}$) is as simple as \"flipping\" it ($Q^T$).\n",
    "* **Example:** $Q = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix}$.\n",
    "    This is a 90Â° counter-clockwise rotation. Let's test it on $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ (the x-axis vector):\n",
    "    $\\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ (the y-axis vector).\n",
    "    It rotated the x-axis to the y-axis, as expected.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ³ Part 2: The \"Recipe\" - Deconstructing $A = U \\Sigma V^T$\n",
    "\n",
    "Now let's look at the three parts of the SVD formula. Remember our story: **Rotate 1, then Scale, then Rotate 2.**\n",
    "\n",
    "#### 1. $V^T$: The First Rotation (The \"Input Aligner\")\n",
    "* **What it is:** $V^T$ is an $n \\times n$ **orthogonal matrix**.\n",
    "* **What it does:** It takes your input vector and **rotates** it.\n",
    "* **Why?** It rotates the input space so that its *most important directions* are perfectly aligned with the standard axes (x, y, z...). These \"most important directions\" are called the **Right Singular Vectors**, and they are the columns of $V$ (the rows of $V^T$).\n",
    "* **Analogy:** Imagine your transformation $A$ is a pasta machine that squashes dough flat. $V^T$ is the first step: *rotating the ball of dough* so it's aligned with the machine's rollers.\n",
    "\n",
    "#### 2. $\\Sigma$: The Scaling (The \"Stretching & Squashing\")\n",
    "* **What it is:** $\\Sigma$ is an $m \\times n$ **diagonal matrix**.\n",
    "* **What it does:** It scales the rotated vector along each axis. The diagonal entries are the **Singular Values** ($\\sigma_1, \\sigma_2, \\sigma_3, ...$).\n",
    "* **Key Details:**\n",
    "    * These values are always positive ($\\ge 0$).\n",
    "    * They are sorted by \"importance\": $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 \\ge ...$\n",
    "    * $\\sigma_1$ is the \"most important\" scaling factor, and $\\sigma_2$ is the second most, and so on.\n",
    "* **Analogy:** This *is* the pasta machine's rollers. It squashes the dough (vector) along the 2nd axis (e.g., $\\sigma_2 = 0.1$) and stretches it along the 1st axis (e.g., $\\sigma_1 = 4$). If a singular value is 0, it means that direction is *completely flattened*.\n",
    "\n",
    "#### 3. $U$: The Second Rotation (The \"Output Aligner\")\n",
    "* **What it is:** $U$ is an $m \\times m$ **orthogonal matrix**.\n",
    "* **What it does:** It takes the stretched-and-squashed vector and **rotates** it to its final position in the output space.\n",
    "* **Why?** The scaling step left our vector aligned with the axes. $U$ rotates it to whatever final orientation the original matrix $A$ required. The columns of $U$ are called the **Left Singular Vectors** and they form the new, principal axes of the *output*.\n",
    "* **Analogy:** The flattened pasta comes out of the machine. $U$ is the step where you *rotate* the sheet of pasta to place it on the drying rack.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¨ Part 3: The Geometric Picture (Circle to Ellipse)\n",
    "\n",
    "This is the most powerful way to understand SVD.\n",
    "\n",
    "**Imagine transforming a unit circle** (all 2D vectors with length 1).\n",
    "\n",
    "\n",
    "\n",
    "1.  **Start:** We have a unit circle. Its main axes are the x-axis $\\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and y-axis $\\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n",
    "2.  **Apply $V^T$ (Rotate 1):** $V^T$ rotates the circle. It's still a perfect circle. But its axes are now aligned with the new **right singular vectors** ($v_1, v_2$).\n",
    "3.  **Apply $\\Sigma$ (Scale):** $\\Sigma$ scales this rotated circle. It stretches by $\\sigma_1$ along the $v_1$ axis and by $\\sigma_2$ along the $v_2$ axis. This transforms the circle into an **ellipse**. The lengths of the ellipse's main axes are exactly $\\sigma_1$ and $\\sigma_2$.\n",
    "4.  **Apply $U$ (Rotate 2):** $U$ takes this new ellipse (which is currently aligned with $v_1, v_2$) and **rotates** it into its final position. The final axes of the ellipse point along the **left singular vectors** ($u_1, u_2$).\n",
    "\n",
    "**SVD, in essence, finds the directions in your input space ($V$) that map perfectly to perpendicular directions in your output space ($U$), and tells you how much each direction was stretched or squashed ($\\Sigma$).**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¬ Part 4: Key Properties & Why SVD is So Useful\n",
    "\n",
    "SVD isn't just a mathematical curiosity; its properties are the reason it's used in data science, image compression, and machine learning.\n",
    "\n",
    "#### 1. Relation to Eigenvalues (The \"Proof\" of SVD)\n",
    "This property shows *how* we find $U$, $\\Sigma$, and $V$.\n",
    "* **Property:** The squares of the singular values ($\\sigma_i^2$) are the **eigenvalues** of the matrix $A^T A$. The columns of $V$ are the **eigenvectors** of $A^T A$.\n",
    "* **Proof Sketch:**\n",
    "    $A = U \\Sigma V^T$\n",
    "    $A^T = (U \\Sigma V^T)^T = V \\Sigma^T U^T$\n",
    "    Let's compute $A^T A$:\n",
    "    $A^T A = (V \\Sigma^T U^T) (U \\Sigma V^T)$\n",
    "    Since $U$ is orthogonal, $U^T U = I$ (the identity matrix), so they cancel out:\n",
    "    $A^T A = V (\\Sigma^T \\Sigma) V^T$\n",
    "    $\\Sigma^T \\Sigma$ is just a square diagonal matrix with $\\sigma_i^2$ on its diagonal. Let's call it $D$.\n",
    "    $A^T A = V D V^T$\n",
    "    This is the *definition* of eigendecomposition. It proves that **$V$ is the matrix of eigenvectors for $A^T A$** and **$D$ is the matrix of its eigenvalues (which are $\\sigma_i^2$)**.\n",
    "    *(A similar proof shows $U$ is the matrix of eigenvectors for $A A^T$)*.\n",
    "* **Example:** If $A = \\begin{bmatrix} 1 & 1 \\\\ 0 & 0 \\end{bmatrix}$, then $A^T A = \\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\end{bmatrix} \\begin{bmatrix} 1 & 1 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$. The eigenvalues of this matrix are $\\lambda_1 = 2$ and $\\lambda_2 = 0$.\n",
    "    This means our singular values are $\\sigma_1 = \\sqrt{2}$ and $\\sigma_2 = \\sqrt{0} = 0$.\n",
    "\n",
    "#### 2. Rank\n",
    "* **Property:** The **rank** of a matrix $A$ (the number of independent dimensions it outputs) is equal to the number of **non-zero singular values**.\n",
    "* **Intuition:** The singular values are the \"magnitudes\" of the transformation's actions. If a singular value is 0, it means the machine *completely flattens* that direction. The rank is the count of directions that *survive* this flattening.\n",
    "* **Example:** For $A = \\begin{bmatrix} 1 & 1 \\\\ 0 & 0 \\end{bmatrix}$, our singular values were $\\sqrt{2}$ and $0$. We have **one** non-zero singular value, so the rank is **1**. This makes sense: the matrix $A$ transforms *every* 2D point onto the x-axis, which is a 1D line.\n",
    "\n",
    "#### 3. Determinant\n",
    "* **Property:** For a square matrix $A$, the absolute value of its determinant is the product of all its singular values: $|\\det(A)| = \\sigma_1 \\cdot \\sigma_2 \\cdot ... \\cdot \\sigma_n$.\n",
    "* **Intuition:** The determinant measures how much a transformation changes *volume* (or area in 2D). The rotations $U$ and $V^T$ don't change volume ($\\det = \\pm 1$). The scaling matrix $\\Sigma$ *is* the change in volume. The new volume is just the product of the scaling factors in each direction ($\\sigma_1 \\times \\sigma_2 \\times ...$).\n",
    "* **Example:** For $A = \\begin{bmatrix} 1 & 1 \\\\ 0 & 0 \\end{bmatrix}$, $\\det(A) = (1)(0) - (1)(0) = 0$. The singular values are $\\sqrt{2}$ and $0$. Their product is $\\sqrt{2} \\cdot 0 = 0$. It matches.\n",
    "\n",
    "#### 4. Low-Rank Approximation (The \"Magic\" Property)\n",
    "* **Property:** The \"best\" rank-$k$ approximation of $A$ is found by taking its SVD and setting all but the largest $k$ singular values to zero.\n",
    "* **The Math:** $A$ can also be written as a sum:\n",
    "    $A = \\sigma_1 u_1 v_1^T + \\sigma_2 u_2 v_2^T + \\sigma_3 u_3 v_3^T + ...$\n",
    "    Since $\\sigma_1 \\ge \\sigma_2 \\ge ...$, the first term is the *most important part* of the matrix, the second term is the next most important, and so on.\n",
    "* **Approximation:** To get a \"good enough\" approximation of $A$, you just add up the first $k$ terms:\n",
    "    $A_k = \\sigma_1 u_1 v_1^T + ... + \\sigma_k u_k v_k^T$\n",
    "* **Application (Image Compression):** An image is just a matrix of pixel values. You compute its SVD. You might find it has 500 singular values. By keeping only the first 50 (the $k=50$ approximation), you can store just those 50 $\\sigma$'s and their corresponding $u, v$ vectors. This uses *far* less data, but the reconstructed image $A_{50}$ looks almost identical to the original. You've thrown away the \"noise\" and \"tiny details\" (the small singular values) and kept the \"important structure.\"\n",
    "\n",
    "## Summary\n",
    "\n",
    "Here are the formal mathematical formulas for the properties of Singular Value Decomposition (SVD).\n",
    "\n",
    "### 1. The SVD Decomposition\n",
    "\n",
    "Let $A$ be an $m \\times n$ matrix.\n",
    "\n",
    "* **Matrix Form:**\n",
    "    $$\n",
    "    A = U \\Sigma V^T\n",
    "    $$\n",
    "    Where $U$ is an $m \\times m$ orthogonal matrix, $\\Sigma$ is an $m \\times n$ diagonal matrix, and $V^T$ is an $n \\times n$ orthogonal matrix.\n",
    "\n",
    "* **Outer Product Form (Sum):**\n",
    "    $$\n",
    "    A = \\sum_{i=1}^{r} \\sigma_i \\vec{u}_i \\vec{v}_i^T\n",
    "    $$\n",
    "    Where $r = \\text{rank}(A)$, $\\sigma_i$ are the singular values, $\\vec{u}_i$ (columns of $U$) are the left singular vectors, and $\\vec{v}_i$ (columns of $V$) are the right singular vectors.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Eigenvalue Relation\n",
    "\n",
    "* **Right Singular Vectors ($V$):** The columns of $V$ ($\\vec{v}_i$) are the eigenvectors of $A^T A$. The eigenvalues of $A^T A$ are the squares of the singular values ($\\sigma_i^2$).\n",
    "    $$\n",
    "    A^T A \\vec{v}_i = \\sigma_i^2 \\vec{v}_i\n",
    "    $$\n",
    "\n",
    "* **Left Singular Vectors ($U$):** The columns of $U$ ($\\vec{u}_i$) are the eigenvectors of $A A^T$. The non-zero eigenvalues of $A A^T$ are also the squares of the singular values ($\\sigma_i^2$).\n",
    "    $$\n",
    "    A A^T \\vec{u}_i = \\sigma_i^2 \\vec{u}_i\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Rank\n",
    "\n",
    "The rank of $A$, denoted $r$, is equal to the number of non-zero singular values.\n",
    "$$\n",
    "\\text{rank}(A) = r = |\\{i \\mid \\sigma_i > 0\\}|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Determinant (for square $n \\times n$ matrices)\n",
    "\n",
    "The absolute value of the determinant of a square matrix $A$ is the product of all its singular values.\n",
    "$$\n",
    "|\\det(A)| = \\prod_{i=1}^{n} \\sigma_i\n",
    "$$\n",
    "\n",
    "More precisely, since $\\det(U) = \\pm 1$ and $\\det(V^T) = \\pm 1$:\n",
    "$$\n",
    "\\det(A) = \\det(U) \\det(V^T) \\prod_{i=1}^{n} \\sigma_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Low-Rank Approximation (Eckart-Young Theorem)\n",
    "\n",
    "The best rank-$k$ approximation of $A$, denoted $A_k$, is the sum of the first $k$ outer products.\n",
    "$$\n",
    "A_k = \\sum_{i=1}^{k} \\sigma_i \\vec{u}_i \\vec{v}_i^T\n",
    "$$\n",
    "This matrix $A_k$ minimizes the Frobenius norm of the difference with $A$:\n",
    "$$\n",
    "A_k = \\underset{B \\text{ s.t. rank}(B)=k}{\\arg\\min} \\|A - B\\|_F\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a8b01a",
   "metadata": {},
   "source": [
    "## Proof Of Properties of SVD With Python, Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9eff56",
   "metadata": {},
   "source": [
    "## âš™ï¸ Part 1: Setup\n",
    "\n",
    "This part just sets up our experiment. We import the `numpy` library (the standard for numerical operations) and define two test matrices.\n",
    "* `A_sq`: A 3x3 **square matrix**. We'll use this to test properties that only apply to square matrices, like the determinant.\n",
    "* `A_rect`: A 4x2 **rectangular matrix**. This is a more general case and is perfect for testing rank and the eigenvalue relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caf74382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original Square Matrix A_sq (3x3) ---\n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8 10]]\n",
      "\n",
      "--- Original Rectangular Matrix A_rect (4x2) ---\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]\n",
      " [7 8]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set print options for cleaner output (fewer decimals, suppress scientific notation)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "# Part 1: Setup\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# A square matrix (for determinant property)\n",
    "A_sq = np.array([[1, 2, 3], \n",
    "                 [4, 5, 6], \n",
    "                 [7, 8, 10]]) \n",
    "\n",
    "# A rectangular matrix (for rank and non-square properties)\n",
    "A_rect = np.array([[1, 2], \n",
    "                   [3, 4], \n",
    "                   [5, 6],\n",
    "                   [7, 8]])\n",
    "\n",
    "print(\"--- Original Square Matrix A_sq (3x3) ---\")\n",
    "print(A_sq)\n",
    "print(\"\\n--- Original Rectangular Matrix A_rect (4x2) ---\")\n",
    "print(A_rect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebea2e9",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Part 2: Proof of Property 1 (Eigenvalues)\n",
    "\n",
    "**Property:** The **eigenvalues** of $A^T A$ are the **squares of the singular values** ($\\sigma^2$).\n",
    "\n",
    "**Explanation of the Proof:**\n",
    "This proof shows the deep connection between eigenvalues and singular values. We test this on our rectangular matrix `A_rect`.\n",
    "\n",
    "1.  **Get Singular Values ($s$):** First, we get the SVD of `A_rect` and find its singular values `s_rect`. We square them to get $\\sigma^2$.\n",
    "2.  **Get Eigenvalues of $A^T A$ ($\\lambda$):** Second, we *independently* calculate the matrix $A^T A$ and find *its* eigenvalues.\n",
    "3.  **Compare:** The `np.allclose` function returns `True`, confirming the two lists of numbers are identical.\n",
    "\n",
    "We then repeat this for $A A^T$ (a larger 4x4 matrix). Its non-zero eigenvalues also perfectly match $\\sigma^2$, with the extra $m-n = 2$ eigenvalues being zero, just as theory predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9747d72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Property 1: SVD and Eigenvalues (using 4x2 matrix A_rect) ---\n",
      "Singular values (s) from SVD:            [14.2691  0.6268]\n",
      "Singular values squared (s^2):           [203.6071   0.3929]\n",
      "\n",
      "Eigenvalues of A_rect.T @ A_rect (2x2):  [203.6071   0.3929]\n",
      "Eigenvalues of A^T@A match s^2:          True\n",
      "\n",
      "Eigenvalues of A_rect @ A_rect.T (4x4):  [203.6071   0.3929  -0.      -0.    ]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Part 2: Proof of Property 1 (Eigenvalues)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\\n--- Property 1: SVD and Eigenvalues (using 4x2 matrix A_rect) ---\")\n",
    "\n",
    "# 1. Get singular values from SVD and square them\n",
    "U_rect, s_rect, Vt_rect = np.linalg.svd(A_rect)\n",
    "s_rect_squared = s_rect**2\n",
    "\n",
    "print(f\"Singular values (s) from SVD:            {s_rect}\")\n",
    "print(f\"Singular values squared (s^2):           {s_rect_squared}\")\n",
    "\n",
    "# 2. Get eigenvalues from A.T @ A\n",
    "AtA = A_rect.T @ A_rect\n",
    "eigvals_AtA, eigvecs_V = np.linalg.eig(AtA)\n",
    "\n",
    "# Sort both descending for comparison\n",
    "s_rect_squared_sorted = np.sort(s_rect_squared)[::-1]\n",
    "eigvals_AtA_sorted = np.sort(np.real(eigvals_AtA))[::-1]\n",
    "\n",
    "print(f\"\\nEigenvalues of A_rect.T @ A_rect (2x2):  {eigvals_AtA_sorted}\")\n",
    "\n",
    "# 3. Compare the results\n",
    "print(f\"Eigenvalues of A^T@A match s^2:          {np.allclose(s_rect_squared_sorted, eigvals_AtA_sorted)}\")\n",
    "\n",
    "# (Optional) Show the same for A @ A.T\n",
    "AAt = A_rect @ A_rect.T\n",
    "eigvals_AAt, eigvecs_U = np.linalg.eig(AAt)\n",
    "eigvals_AAt_sorted = np.sort(np.real(eigvals_AAt))[::-1] \n",
    "print(f\"\\nEigenvalues of A_rect @ A_rect.T (4x4):  {eigvals_AAt_sorted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5624197",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Part 3: Proof of Property 2 (Determinant)\n",
    "\n",
    "**Property:** For a square matrix, the **absolute value of the determinant** is the **product of the singular values**.\n",
    "\n",
    "**Explanation of the Proof:**\n",
    "This proof uses our square matrix `A_sq`.\n",
    "\n",
    "1.  **Direct Determinant:** We use `np.linalg.det(A_sq)` to find the determinant *directly*.\n",
    "2.  **SVD Product:** We get the singular values `s_sq` and multiply them all together using `np.prod(s_sq)`.\n",
    "3.  **Compare:** The values are identical. This proves that the determinant (which measures the \"volume change\" of a transformation) is fundamentally just the product of the scaling factors ($\\sigma_i$) from the SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd4742a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Property 2: Determinant (using 3x3 matrix A_sq) ---\n",
      "Direct calculation: |det(A_sq)| =       3.0000\n",
      "From SVD: product(s) =                3.0000\n",
      "|det(A)| matches product(s):           True\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Part 3: Proof of Property 2 (Determinant)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\\n--- Property 2: Determinant (using 3x3 matrix A_sq) ---\")\n",
    "\n",
    "# Get SVD components\n",
    "U_sq, s_sq, Vt_sq = np.linalg.svd(A_sq)\n",
    "\n",
    "# 1. Direct calculation\n",
    "det_direct = np.linalg.det(A_sq)\n",
    "abs_det_direct = np.abs(det_direct)\n",
    "print(f\"Direct calculation: |det(A_sq)| =       {abs_det_direct:.4f}\")\n",
    "\n",
    "# 2. SVD calculation\n",
    "prod_s = np.prod(s_sq)\n",
    "print(f\"From SVD: product(s) =                {prod_s:.4f}\")\n",
    "\n",
    "# 3. Compare\n",
    "print(f\"|det(A)| matches product(s):           {np.allclose(abs_det_direct, prod_s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90535d6f",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Part 4: Proof of Property 3 (Rank)\n",
    "\n",
    "**Property:** The **rank** of a matrix is the number of **non-zero singular values**.\n",
    "\n",
    "**Explanation of the Proof:**\n",
    "This proof uses our rectangular matrix `A_rect`.\n",
    "\n",
    "1.  **Direct Rank:** We use `np.linalg.matrix_rank(A_rect)` to ask NumPy for the rank directly.\n",
    "2.  **SVD Rank:** We look at the singular values `s_rect`. We count how many are greater than a tiny tolerance (to avoid floating-point errors).\n",
    "3.  **Compare:** Both methods agree. The rank (the number of \"dimensions\" in the output) is exactly the number of non-zero scaling factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3e903f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Property 3: Rank (using 4x2 matrix A_rect) ---\n",
      "Direct rank from np.linalg.matrix_rank: 2\n",
      "Rank from SVD (count of s > tol):    2\n",
      "Ranks match:                         True\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Part 4: Proof of Property 3 (Rank)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\\n--- Property 3: Rank (using 4x2 matrix A_rect) ---\")\n",
    "\n",
    "# 1. Direct rank\n",
    "rank_direct = np.linalg.matrix_rank(A_rect)\n",
    "print(f\"Direct rank from np.linalg.matrix_rank: {rank_direct}\")\n",
    "\n",
    "# 2. Rank from SVD\n",
    "# (s_rect was [14.2691  0.6268])\n",
    "tol = np.finfo(s_rect.dtype).eps * max(A_rect.shape)\n",
    "rank_from_svd = np.sum(s_rect > tol)\n",
    "print(f\"Rank from SVD (count of s > tol):    {rank_from_svd}\")\n",
    "\n",
    "# 3. Compare\n",
    "print(f\"Ranks match:                         {rank_direct == rank_from_svd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee72980",
   "metadata": {},
   "source": [
    "## ðŸ”¬ Part 5: Proof of Property 4 (Low-Rank Approximation)\n",
    "\n",
    "**Property:** The best **rank-k approximation** of $A$ is $A_k = U \\Sigma_k V^T$, where $\\Sigma_k$ is the $\\Sigma$ matrix with all but the $k$ largest singular values set to zero.\n",
    "\n",
    "**Explanation of the Proof:**\n",
    "This proof is in two parts. First, we show that $U \\Sigma V^T$ perfectly rebuilds $A$. Second, we show that $U \\Sigma_k V^T$ creates a new matrix of rank $k$.\n",
    "\n",
    "1.  **Part 1: Reconstruction:** We use the \"economy\" SVD for easier math. We take `U_econ`, `s_econ`, and `Vt_econ`, reassemble them (turning the `s_econ` array into a 2D `Sigma_full` matrix), and multiply them. This proves $A = U \\Sigma V^T$ is true.\n",
    "2.  **Part 2: Approximation:** We decide we want a **rank-1** approximation (so $k=1$).\n",
    "    * We copy the singular values `s_econ`.\n",
    "    * We set all but the first $k=1$ values to zero, giving `s_k = [14.2691, 0.]`.\n",
    "    * We build a new matrix, `A_k1`, using these *modified* singular values.\n",
    "3.  **Verify Rank:** We check the rank of our new matrix `A_k1` using `np.linalg.matrix_rank`.\n",
    "    * Output: The rank is **1**. This proves that the SVD method successfully created a new matrix of exactly the rank we asked for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "866aecf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Property 4: Low-Rank Approximation (using 4x2 matrix A_rect) ---\n",
      "\n",
      "Full Reconstructed A_rect (from economy SVD):\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]\n",
      " [7. 8.]]\n",
      "Reconstruction matches original: True\n",
      "\n",
      "Rank-1 Approximation (A_k1):\n",
      "[[1.3956 1.6692]\n",
      " [3.2026 3.8306]\n",
      " [5.0097 5.9919]\n",
      " [6.8167 8.1533]]\n",
      "Rank of A_k1 is: 1 (as expected)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# Part 5: Proof of Property 4 (Low-Rank Approximation)\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(\"\\n\\n--- Property 4: Low-Rank Approximation (using 4x2 matrix A_rect) ---\")\n",
    "\n",
    "# Use 'economy' SVD for easier reconstruction\n",
    "U_econ, s_econ, Vt_econ = np.linalg.svd(A_rect, full_matrices=False)\n",
    "# U_econ is 4x2, s_econ is (2,), Vt_econ is 2x2.\n",
    "\n",
    "# 1. --- Part 1: Prove Reconstruction Works ---\n",
    "Sigma_full = np.diag(s_econ)\n",
    "A_recon = U_econ @ Sigma_full @ Vt_econ\n",
    "print(\"\\nFull Reconstructed A_rect (from economy SVD):\")\n",
    "print(A_recon)\n",
    "print(f\"Reconstruction matches original: {np.allclose(A_rect, A_recon)}\")\n",
    "\n",
    "\n",
    "# 2. --- Part 2: Create a Rank-1 Approximation ---\n",
    "k = 1\n",
    "s_k = np.copy(s_econ)\n",
    "s_k[k:] = 0 # Zero out all but the first k singular values\n",
    "Sigma_k = np.diag(s_k)\n",
    "\n",
    "A_k1 = U_econ @ Sigma_k @ Vt_econ\n",
    "print(f\"\\nRank-{k} Approximation (A_k1):\")\n",
    "print(A_k1)\n",
    "\n",
    "# 3. --- Verify the Rank of the New Matrix ---\n",
    "rank_k1 = np.linalg.matrix_rank(A_k1)\n",
    "print(f\"Rank of A_k1 is: {rank_k1} (as expected)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor-geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
