{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2272e336",
   "metadata": {},
   "source": [
    "# ðŸ§  What is Covariance?\n",
    "\n",
    "In simple terms, **covariance** is a statistical measure that describes the **directional relationship** between two random variables. It tells you whether two variables tend to move together or in opposite directions.\n",
    "\n",
    "* **Positive Covariance:** Indicates that as one variable increases, the other variable tends to **increase**.\n",
    "* **Negative Covariance:** Indicates that as one variable increases, the other variable tends to **decrease**.\n",
    "* **Zero Covariance:** Indicates that there is no linear relationship between the two variables.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® The Mathematics: Formulas\n",
    "\n",
    "The formula for covariance differs slightly depending on whether you are calculating it for an entire **population** or for a **sample**.\n",
    "\n",
    "Let $X$ and $Y$ be two random variables, with:\n",
    "* $n$ = number of observations\n",
    "* $x_i, y_i$ = the individual data points\n",
    "* $\\mu_X, \\mu_Y$ = the population means of $X$ and $Y$\n",
    "* $\\bar{x}, \\bar{y}$ = the sample means of $X$ and $Y$\n",
    "* $E[...]$ = the Expected Value (the theoretical mean)\n",
    "\n",
    "#### 1. Population Covariance\n",
    "\n",
    "This is the theoretical covariance between two random variables, $X$ and $Y$.\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(X, Y) = E[ (X - \\mu_X)(Y - \\mu_Y) ]\n",
    "$$\n",
    "\n",
    "A common computational form of this formula is:\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(X, Y) = E[XY] - E[X]E[Y]\n",
    "$$\n",
    "\n",
    "#### 2. Sample Covariance\n",
    "\n",
    "This is the formula you use when calculating covariance from a set of data (a sample).\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(x, y) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n",
    "$$\n",
    "\n",
    "> **Why divide by $n-1$?**\n",
    "> This is known as **Bessel's correction**. It corrects the bias in the sample covariance, making it a better and more accurate estimator of the true population covariance, especially with smaller samples.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Key Properties of Covariance\n",
    "\n",
    "Let $X$, $Y$, and $Z$ be random variables and $a, b$ be constants.\n",
    "\n",
    "1.  **Symmetry:** The order of the variables does not matter.\n",
    "    $$\n",
    "    \\mathrm{Cov}(X, Y) = \\mathrm{Cov}(Y, X)\n",
    "    $$\n",
    "\n",
    "2.  **Relationship with Variance:** The covariance of a variable with itself is its **variance**.\n",
    "    $$\n",
    "    \\mathrm{Cov}(X, X) = \\mathrm{Var}(X)\n",
    "    $$\n",
    "\n",
    "3.  **Effect of Adding a Constant:** Adding a constant (a fixed number) to a variable does not change its covariance, because it doesn't change its \"spread.\"\n",
    "    $$\n",
    "    \\mathrm{Cov}(X + a, Y) = \\mathrm{Cov}(X, Y)\n",
    "    $$\n",
    "\n",
    "4.  **Effect of Scaling (Multiplying by a Constant):** Multiplying a variable by a constant scales the covariance by that same constant.\n",
    "    $$\n",
    "    \\mathrm{Cov}(aX, Y) = a \\cdot \\mathrm{Cov}(X, Y)\n",
    "    $$\n",
    "\n",
    "5.  **Bilinearity:** This combines the scaling and additivity properties.\n",
    "    * $\\mathrm{Cov}(aX + bY, Z) = a \\cdot \\mathrm{Cov}(X, Z) + b \\cdot \\mathrm{Cov}(Y, Z)$\n",
    "    * This is fundamental to how covariance matrices are used in linear algebra.\n",
    "\n",
    "6.  **Covariance of Sums:** The covariance of a sum of variables is the sum of all their individual covariances.\n",
    "    $$\n",
    "    \\mathrm{Cov}(X + Y, Z) = \\mathrm{Cov}(X, Z) + \\mathrm{Cov}(Y, Z)\n",
    "    $$\n",
    "    Similarly, this allows us to find the variance of a sum:\n",
    "    $$\n",
    "    \\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) + 2 \\cdot \\mathrm{Cov}(X, Y)\n",
    "    $$\n",
    "\n",
    "7.  **Independence:** If two variables $X$ and $Y$ are **independent**, their covariance is **zero**.\n",
    "    \n",
    "    If $X, Y$ are independent, then $\\mathrm{Cov}(X, Y) = 0$.\n",
    "\n",
    "    > **Important:** The reverse is **not** always true. A covariance of zero only means there is no *linear* relationship. Two variables can have a strong non-linear relationship (like a parabola) and still have zero covariance.\n",
    "\n",
    "---\n",
    "\n",
    "## Covariance vs. Variance vs. Correlation\n",
    "\n",
    "This table helps clarify the differences between these related concepts.\n",
    "\n",
    "| Measure | What It Measures | Units | Range |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Variance** | The spread of a **single variable** around its mean. (How much does it vary?) | $\\mathrm{Units}^2$ | $0$ to $+\\infty$ |\n",
    "| **Covariance** | The **directional relationship** between **two variables**. (Do they move together?) | $\\mathrm{Units\\ of\\ } X \\times \\mathrm{Units\\ of\\ } Y$ | $-\\infty$ to $+\\infty$ |\n",
    "| **Correlation** | The **strength and direction** of the linear relationship between **two variables**. | None (Standardized) | $-1$ to $+1$ |\n",
    "\n",
    "**Key Limitation of Covariance:**\n",
    "The main weakness of covariance is that its value is **not standardized**. A covariance of 100 might be very large for one dataset but tiny for another, depending on the units (e.g., dollars vs. cents). This makes it hard to compare the \"strength\" of relationships.\n",
    "\n",
    "**How Correlation \"Fixes\" This:**\n",
    "**Correlation** is simply the **normalized version of covariance**. You calculate it by dividing the covariance by the standard deviations of both variables.\n",
    "\n",
    "$$\n",
    "\\mathrm{Corr}(X, Y) = \\rho_{XY} = \\frac{\\mathrm{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n",
    "$$\n",
    "\n",
    "This standardization scales the value to a range of **-1 to +1**, allowing you to directly compare the *strength* of the linear relationship between different pairs of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d0184",
   "metadata": {},
   "source": [
    "# ðŸ§® Steps to Calculate the Covariance Matrix\n",
    "\n",
    "## 1. Define the Data Matrix\n",
    "\n",
    "Suppose you have a data matrix $X$ with shape $n \\times m$, where:\n",
    "* $n$ = number of **observations (samples)**\n",
    "* $m$ = number of **variables (features)**\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "2.1 & 8.0 \\\\\n",
    "2.5 & 12.0 \\\\\n",
    "3.6 & 14.0 \\\\\n",
    "4.0 & 10.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, we have:\n",
    "* $n = 4$ samples\n",
    "* $m = 2$ variables (letâ€™s call them $X_1$ and $X_2$)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Compute the Mean of Each Variable\n",
    "\n",
    "First, calculate the mean (average) for each variable (column).\n",
    "\n",
    "**Mean of $X_1$:**\n",
    "$$\n",
    "\\bar{X}_1 = \\frac{2.1 + 2.5 + 3.6 + 4.0}{4} = \\frac{12.2}{4} = 3.05\n",
    "$$\n",
    "\n",
    "**Mean of $X_2$:**\n",
    "$$\n",
    "\\bar{X}_2 = \\frac{8 + 12 + 14 + 10}{4} = \\frac{44}{4} = 11\n",
    "$$\n",
    "\n",
    "This gives us a **mean vector** $\\bar{X}$:\n",
    "$$\n",
    "\\bar{X} = [3.05, 11]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Subtract the Mean (Mean-Centering)\n",
    "\n",
    "Next, create a new matrix by subtracting the corresponding variable's mean from each value. This process is called **mean-centering** the data.\n",
    "\n",
    "$$\n",
    "X_c = X - \\bar{X} =\n",
    "\\begin{bmatrix}\n",
    "2.1 - 3.05 & 8 - 11 \\\\\n",
    "2.5 - 3.05 & 12 - 11 \\\\\n",
    "3.6 - 3.05 & 14 - 11 \\\\\n",
    "4.0 - 3.05 & 10 - 11\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-0.95 & -3 \\\\\n",
    "-0.55 & 1 \\\\\n",
    "0.55 & 3 \\\\\n",
    "0.95 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Compute the Covariance Matrix\n",
    "\n",
    "The formula for the **sample covariance matrix** ($\\Sigma$) is:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n - 1} (X_c)^T (X_c)\n",
    "$$\n",
    "\n",
    "Where $(X_c)^T$ is the transpose of the centered matrix.\n",
    "\n",
    "**Step 4a: Transpose the centered matrix**\n",
    "\n",
    "$$\n",
    "(X_c)^T =\n",
    "\\begin{bmatrix}\n",
    "-0.95 & -0.55 & 0.55 & 0.95 \\\\\n",
    "-3 & 1 & 3 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 4b: Multiply the transpose by the centered matrix**\n",
    "\n",
    "$$\n",
    "(X_c)^T (X_c) =\n",
    "\\begin{bmatrix}\n",
    "-0.95 & -0.55 & 0.55 & 0.95 \\\\\n",
    "-3 & 1 & 3 & -1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "-0.95 & -3 \\\\\n",
    "-0.55 & 1 \\\\\n",
    "0.55 & 3 \\\\\n",
    "0.95 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's compute the four elements of the resulting matrix:\n",
    "\n",
    "* **Top-left (Var $X_1$):**\n",
    "    $(-0.95)^2 + (-0.55)^2 + (0.55)^2 + (0.95)^2 = 0.9025 + 0.3025 + 0.3025 + 0.9025 = 2.41$\n",
    "* **Bottom-right (Var $X_2$):**\n",
    "    $(-3)^2 + (1)^2 + (3)^2 + (-1)^2 = 9 + 1 + 9 + 1 = 20$\n",
    "* **Top-right (Cov $X_1, X_2$):**\n",
    "    $(-0.95)(-3) + (-0.55)(1) + (0.55)(3) + (0.95)(-1) = 2.85 - 0.55 + 1.65 - 0.95 = 3.0$\n",
    "* **Bottom-left (Cov $X_2, X_1$):**\n",
    "    $(-3)(-0.95) + (1)(-0.55) + (3)(0.55) + (-1)(0.95) = 2.85 - 0.55 + 1.65 - 0.95 = 3.0$\n",
    "\n",
    "This gives the matrix of \"sum of squares\":\n",
    "$$\n",
    "(X_c)^T (X_c) =\n",
    "\\begin{bmatrix}\n",
    "2.41 & 3.0 \\\\\n",
    "3.0 & 20\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 4c: Divide by $n - 1$**\n",
    "\n",
    "We divide by $n - 1 = 4 - 1 = 3$ (this is the degrees of freedom for a *sample* covariance).\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{3}\n",
    "\\begin{bmatrix}\n",
    "2.41 & 3.0 \\\\\n",
    "3.0 & 20\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2.41 / 3 & 3.0 / 3 \\\\\n",
    "3.0 / 3 & 20 / 3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.8033... & 1.0 \\\\\n",
    "1.0 & 6.666...\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Final Covariance Matrix\n",
    "\n",
    "Rounding to four decimal places, the covariance matrix is:\n",
    "\n",
    "$$\n",
    "\\Sigma =\n",
    "\\begin{bmatrix}\n",
    "0.8033 & 1.0 \\\\\n",
    "1.0 & 6.6667\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Interpretation\n",
    "\n",
    "The covariance matrix $\\Sigma = \\begin{bmatrix} \\sigma^2(X_1) & \\text{Cov}(X_1, X_2) \\\\ \\text{Cov}(X_2, X_1) & \\sigma^2(X_2) \\end{bmatrix}$ tells us:\n",
    "\n",
    "* **Diagonal (Variances):**\n",
    "    * **Variance of $X_1$:** $\\sigma^2(X_1) \\approx 0.8033$. This measures the spread of the first variable.\n",
    "    * **Variance of $X_2$:** $\\sigma^2(X_2) \\approx 6.6667$. The second variable is much more spread out than the first.\n",
    "* **Off-Diagonal (Covariance):**\n",
    "    * **Covariance between $X_1$ and $X_2$:** $\\text{Cov}(X_1, X_2) = 1.0$.\n",
    "    * Since the covariance is **positive**, it indicates that as $X_1$ increases, $X_2$ tends to increase as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36363c",
   "metadata": {},
   "source": [
    "## Covariance Properties and Calculation With Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab635f7a",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import NumPy and create our sample data. We'll create three variables, $X$, $Y$, and $Z$, with 5 samples each. We'll also define our constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8f219a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: [1 2 4 5 6]\n",
      "Y: [2 3 5 4 6]\n",
      "Z: [8 6 7 4 3]\n",
      "a: 5, b: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define our variables (as 1D arrays)\n",
    "X = np.array([1, 2, 4, 5, 6])  # e.g., Study Hours\n",
    "Y = np.array([2, 3, 5, 4, 6])  # e.g., Test Score\n",
    "Z = np.array([8, 6, 7, 4, 3])  # e.g., Game Hours\n",
    "\n",
    "# Define constants\n",
    "a = 5\n",
    "b = 10\n",
    "\n",
    "print(f\"X: {X}\")\n",
    "print(f\"Y: {Y}\")\n",
    "print(f\"Z: {Z}\")\n",
    "print(f\"a: {a}, b: {b}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a8ea0",
   "metadata": {},
   "source": [
    "## 2. Baseline Calculations:\n",
    "\n",
    "Instead of calculating covariances one by one, the NumPy way is to compute the full covariance matrix. We do this by stacking our variables into a single data matrix (with columns as variables) and passing it to ``np.cov()``.\n",
    "\n",
    "**Note:**\n",
    "- ``rowvar=False`` tells NumPy that our **columns** are variables (the standard).\n",
    "- ``ddof=1`` tells NumPy to use the **sample** covariance formula (dividing by $n-1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b886d096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Matrix (samples=rows, variables=cols):\n",
      " [[1 2 8]\n",
      " [2 3 6]\n",
      " [4 5 7]\n",
      " [5 4 4]\n",
      " [6 6 3]]\n",
      "\n",
      "Full Covariance Matrix:\n",
      " [[ 4.3   3.   -3.7 ]\n",
      " [ 3.    2.5  -2.25]\n",
      " [-3.7  -2.25  4.3 ]]\n"
     ]
    }
   ],
   "source": [
    "# Stack X, Y, Z as columns in one data matrix\n",
    "# (samples, variables) -> (5, 3)\n",
    "data = np.stack([X, Y, Z], axis=1)\n",
    "\n",
    "print(\"Data Matrix (samples=rows, variables=cols):\\n\", data)\n",
    "\n",
    "# Calculate the 3x3 covariance matrix\n",
    "cov_matrix = np.cov(data, rowvar=False, ddof=1)\n",
    "\n",
    "print(\"\\nFull Covariance Matrix:\\n\", np.round(cov_matrix, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453197b1",
   "metadata": {},
   "source": [
    "### 2.1. Let's grab our baseline values from this matrix.\n",
    "Extract baseline values for our proofs\n",
    "\n",
    "$$\\mathbf{\\Sigma} =\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\n",
    "\\text{Var}(X) & \\text{Cov}(X, Y) & \\text{Cov}(X, Z) \\\\\n",
    "\n",
    "\\text{Cov}(X, Y) & \\text{Var}(Y) & \\text{Cov}(Y, Z) \\\\\n",
    "\n",
    "\\text{Cov}(X, Z) & \\text{Cov}(Y, Z) & \\text{Var}(Z)\n",
    "\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71609c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(X): 4.3000\n",
      "Var(Y): 2.5000\n",
      "Var(Z): 4.3000\n",
      "Cov(X, Y): 3.0000\n",
      "Cov(X, Z): -3.7000\n",
      "Cov(Y, Z): -2.2500\n"
     ]
    }
   ],
   "source": [
    "var_X = cov_matrix[0, 0]\n",
    "var_Y = cov_matrix[1, 1]\n",
    "var_Z = cov_matrix[2, 2]\n",
    "\n",
    "cov_XY = cov_matrix[0, 1]\n",
    "cov_XZ = cov_matrix[0, 2]\n",
    "cov_YZ = cov_matrix[1, 2]\n",
    "\n",
    "print(f\"Var(X): {var_X:.4f}\")\n",
    "print(f\"Var(Y): {var_Y:.4f}\")\n",
    "print(f\"Var(Z): {var_Z:.4f}\")\n",
    "print(f\"Cov(X, Y): {cov_XY:.4f}\")\n",
    "print(f\"Cov(X, Z): {cov_XZ:.4f}\")\n",
    "print(f\"Cov(Y, Z): {cov_YZ:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaca8b9",
   "metadata": {},
   "source": [
    "## Property 1: SymmetryFormula:\n",
    "\n",
    "**Formula:** $\\text{Cov}(X, Y) = \\text{Cov}(Y, X)$\n",
    "\n",
    "**Explanation:** The covariance matrix is always symmetric. ``cov_matrix[i, j]`` is always equal to ``cov_matrix[j, i]``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3802127c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Property 1: Symmetry ###\n",
      "  LHS (Cov(X, Y)): 3.0000\n",
      "  RHS (Cov(Y, X)): 3.0000\n",
      "  Property holds: True\n"
     ]
    }
   ],
   "source": [
    "print(\"### Property 1: Symmetry ###\")\n",
    "# LHS is Cov(X, Y), RHS is Cov(Y, X)\n",
    "lhs = cov_matrix[0, 1] # Cov(X, Y)\n",
    "rhs = cov_matrix[1, 0] # Cov(Y, X)\n",
    "\n",
    "print(f\"  LHS (Cov(X, Y)): {lhs:.4f}\")\n",
    "print(f\"  RHS (Cov(Y, X)): {rhs:.4f}\")\n",
    "print(f\"  Property holds: {np.isclose(lhs, rhs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21caed1f",
   "metadata": {},
   "source": [
    "## Property 2: Covariance with Itself\n",
    "\n",
    "**Formula:** $\\text{Cov}(X, X) = \\text{Var}(X)$\n",
    "\n",
    "**Explanation:** This is why the diagonal of the covariance matrix is the variance of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f4f5f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Property 2: Cov(X, X) = Var(X) ###\n",
      "  LHS (Cov(X, X)): 4.3000\n",
      "  RHS (Var(X)):    4.3000\n",
      "  Property holds: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### Property 2: Cov(X, X) = Var(X) ###\")\n",
    "# LHS: Cov(X, X) from the matrix diagonal\n",
    "lhs = cov_matrix[0, 0]\n",
    "\n",
    "# RHS: Var(X) calculated separately (or just looked up)\n",
    "rhs = np.var(X, ddof=1)\n",
    "\n",
    "print(f\"  LHS (Cov(X, X)): {lhs:.4f}\")\n",
    "print(f\"  RHS (Var(X)):    {rhs:.4f}\")\n",
    "print(f\"  Property holds: {np.isclose(lhs, rhs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aa56f1",
   "metadata": {},
   "source": [
    "## Property 3: Effect of Adding a Constant (Shift)\n",
    "\n",
    "**Formula:** $\\text{Cov}(X + a, Y) = \\text{Cov}(X, Y)$\n",
    "\n",
    "**Explanation:** Shifting the data (adding a constant) changes its mean but not its spread (variance) or its relationship with other variables (covariance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bf8e08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Property 3: Cov(X + 5, Y) = Cov(X, Y) ###\n",
      "  New Var (X + 5): [ 6  7  9 10 11]\n",
      "  LHS (Cov(X + 5, Y)): 3.0000\n",
      "  RHS (Cov(X, Y)):   3.0000\n",
      "  Property holds: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n### Property 3: Cov(X + {a}, Y) = Cov(X, Y) ###\")\n",
    "# LHS: Calculate covariance with the new, shifted variable\n",
    "X_plus_a = X + a\n",
    "# We must re-calculate covariance for this new variable\n",
    "lhs = np.cov(X_plus_a, Y, ddof=1)[0, 1]\n",
    "\n",
    "# RHS: Our original, baseline Cov(X, Y)\n",
    "rhs = cov_XY\n",
    "\n",
    "print(f\"  New Var (X + {a}): {X_plus_a}\")\n",
    "print(f\"  LHS (Cov(X + {a}, Y)): {lhs:.4f}\")\n",
    "print(f\"  RHS (Cov(X, Y)):   {rhs:.4f}\")\n",
    "print(f\"  Property holds: {np.isclose(lhs, rhs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ed2a8",
   "metadata": {},
   "source": [
    "## Property 4: Effect of ScalingFormula: \n",
    "\n",
    "**Formula:** $\\text{Cov}(aX, Y) = a \\cdot \\text{Cov}(X, Y)$\n",
    "\n",
    "**Explanation:** Scaling a variable by $a$ directly scales its covariance with other variables by $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8311a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Property 4: Cov(5*X, Y) = 5*Cov(X, Y) ###\n",
      "  New Var (5*X): [ 5 10 20 25 30]\n",
      "  LHS (Cov(5*X, Y)): 15.0000\n",
      "  RHS (5 * Cov(X, Y)): 15.0000\n",
      "  Property holds: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n### Property 4: Cov({a}*X, Y) = {a}*Cov(X, Y) ###\")\n",
    "# LHS: Calculate covariance with the new, scaled variable\n",
    "aX = a * X\n",
    "lhs = np.cov(aX, Y, ddof=1)[0, 1]\n",
    "\n",
    "# RHS: Our original Cov(X, Y), scaled by the constant\n",
    "rhs = a * cov_XY\n",
    "\n",
    "print(f\"  New Var ({a}*X): {aX}\")\n",
    "print(f\"  LHS (Cov({a}*X, Y)): {lhs:.4f}\")\n",
    "print(f\"  RHS ({a} * Cov(X, Y)): {rhs:.4f}\")\n",
    "print(f\"  Property holds: {np.isclose(lhs, rhs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f2d67b",
   "metadata": {},
   "source": [
    "## Property 5: Bilinearity (Distributive)\n",
    "\n",
    "**Formula:** $\\text{Cov}(aX + bY, Z) = a \\cdot \\text{Cov}(X, Z) + b \\cdot \\text{Cov}(Y, Z)$\n",
    "\n",
    "**Explanation:** This shows we can \"distribute\" the covariance operation over linear combinations. This is a very powerful property used in portfolio math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87c22950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Property 5: Bilinearity ###\n",
      "  Test: Cov(5X + 10Y, Z) = 5*Cov(X,Z) + 10*Cov(Y,Z)\n",
      "  New Var (5X + 10Y): [25 40 70 65 90]\n",
      "  LHS (Cov(new, Z)): -41.0000\n",
      "  RHS (a*Cov(X,Z) + b*Cov(Y,Z)): -41.0000\n",
      "  Property holds: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n### Property 5: Bilinearity ###\")\n",
    "print(f\"  Test: Cov({a}X + {b}Y, Z) = {a}*Cov(X,Z) + {b}*Cov(Y,Z)\")\n",
    "\n",
    "# LHS: Create the new combined variable and find its covariance with Z\n",
    "new_var = a * X + b * Y\n",
    "lhs = np.cov(new_var, Z, ddof=1)[0, 1]\n",
    "\n",
    "# RHS: Use our baseline values and combine them\n",
    "rhs = a * cov_XZ + b * cov_YZ\n",
    "\n",
    "print(f\"  New Var ({a}X + {b}Y): {new_var}\")\n",
    "print(f\"  LHS (Cov(new, Z)): {lhs:.4f}\")\n",
    "print(f\"  RHS (a*Cov(X,Z) + b*Cov(Y,Z)): {rhs:.4f}\")\n",
    "print(f\"  Property holds: {np.isclose(lhs, rhs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac01b1",
   "metadata": {},
   "source": [
    "## Property 6: Variance of a Sum\n",
    "\n",
    "**Formula:** $\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2 \\cdot \\text{Cov}(X, Y)$\n",
    "\n",
    "**Explanation:** This is a crucial property. The variance of a sum is **not** just the sum of the variances. You must account for the covariance between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0706c82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Property 6: Variance of a Sum ###\n",
      "  Test: Var(X + Y) = Var(X) + Var(Y) + 2*Cov(X, Y)\n",
      "  New Var (X + Y): [ 3  5  9  9 12]\n",
      "  LHS (Var(X + Y)): 12.8000\n",
      "  RHS (Var(X)+Var(Y)+2*Cov(X,Y)): 12.8000\n",
      "  Property holds: True\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### Property 6: Variance of a Sum ###\")\n",
    "print(\"  Test: Var(X + Y) = Var(X) + Var(Y) + 2*Cov(X, Y)\")\n",
    "\n",
    "# LHS: Create the new variable (X + Y) and find its variance\n",
    "X_plus_Y = X + Y\n",
    "lhs = np.var(X_plus_Y, ddof=1)\n",
    "\n",
    "# RHS: Combine our baseline components\n",
    "rhs = var_X + var_Y + 2 * cov_XY\n",
    "\n",
    "print(f\"  New Var (X + Y): {X_plus_Y}\")\n",
    "print(f\"  LHS (Var(X + Y)): {lhs:.4f}\")\n",
    "print(f\"  RHS (Var(X)+Var(Y)+2*Cov(X,Y)): {rhs:.4f}\")\n",
    "print(f\"  Property holds: {np.isclose(lhs, rhs)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor-geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
