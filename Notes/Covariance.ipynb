{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2272e336",
   "metadata": {},
   "source": [
    "# ðŸ§  What is Covariance?\n",
    "\n",
    "In simple terms, **covariance** is a statistical measure that describes the **directional relationship** between two random variables. It tells you whether two variables tend to move together or in opposite directions.\n",
    "\n",
    "* **Positive Covariance:** Indicates that as one variable increases, the other variable tends to **increase**.\n",
    "* **Negative Covariance:** Indicates that as one variable increases, the other variable tends to **decrease**.\n",
    "* **Zero Covariance:** Indicates that there is no linear relationship between the two variables.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® The Mathematics: Formulas\n",
    "\n",
    "The formula for covariance differs slightly depending on whether you are calculating it for an entire **population** or for a **sample**.\n",
    "\n",
    "Let $X$ and $Y$ be two random variables, with:\n",
    "* $n$ = number of observations\n",
    "* $x_i, y_i$ = the individual data points\n",
    "* $\\mu_X, \\mu_Y$ = the population means of $X$ and $Y$\n",
    "* $\\bar{x}, \\bar{y}$ = the sample means of $X$ and $Y$\n",
    "* $E[...]$ = the Expected Value (the theoretical mean)\n",
    "\n",
    "#### 1. Population Covariance\n",
    "\n",
    "This is the theoretical covariance between two random variables, $X$ and $Y$.\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(X, Y) = E[ (X - \\mu_X)(Y - \\mu_Y) ]\n",
    "$$\n",
    "\n",
    "A common computational form of this formula is:\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(X, Y) = E[XY] - E[X]E[Y]\n",
    "$$\n",
    "\n",
    "#### 2. Sample Covariance\n",
    "\n",
    "This is the formula you use when calculating covariance from a set of data (a sample).\n",
    "\n",
    "$$\n",
    "\\mathrm{Cov}(x, y) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n",
    "$$\n",
    "\n",
    "> **Why divide by $n-1$?**\n",
    "> This is known as **Bessel's correction**. It corrects the bias in the sample covariance, making it a better and more accurate estimator of the true population covariance, especially with smaller samples.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Key Properties of Covariance\n",
    "\n",
    "Let $X$, $Y$, and $Z$ be random variables and $a, b$ be constants.\n",
    "\n",
    "1.  **Symmetry:** The order of the variables does not matter.\n",
    "    $$\n",
    "    \\mathrm{Cov}(X, Y) = \\mathrm{Cov}(Y, X)\n",
    "    $$\n",
    "\n",
    "2.  **Relationship with Variance:** The covariance of a variable with itself is its **variance**.\n",
    "    $$\n",
    "    \\mathrm{Cov}(X, X) = \\mathrm{Var}(X)\n",
    "    $$\n",
    "\n",
    "3.  **Effect of Adding a Constant:** Adding a constant (a fixed number) to a variable does not change its covariance, because it doesn't change its \"spread.\"\n",
    "    $$\n",
    "    \\mathrm{Cov}(X + a, Y) = \\mathrm{Cov}(X, Y)\n",
    "    $$\n",
    "\n",
    "4.  **Effect of Scaling (Multiplying by a Constant):** Multiplying a variable by a constant scales the covariance by that same constant.\n",
    "    $$\n",
    "    \\mathrm{Cov}(aX, Y) = a \\cdot \\mathrm{Cov}(X, Y)\n",
    "    $$\n",
    "\n",
    "5.  **Bilinearity:** This combines the scaling and additivity properties.\n",
    "    * $\\mathrm{Cov}(aX + bY, Z) = a \\cdot \\mathrm{Cov}(X, Z) + b \\cdot \\mathrm{Cov}(Y, Z)$\n",
    "    * This is fundamental to how covariance matrices are used in linear algebra.\n",
    "\n",
    "6.  **Covariance of Sums:** The covariance of a sum of variables is the sum of all their individual covariances.\n",
    "    $$\n",
    "    \\mathrm{Cov}(X + Y, Z) = \\mathrm{Cov}(X, Z) + \\mathrm{Cov}(Y, Z)\n",
    "    $$\n",
    "    Similarly, this allows us to find the variance of a sum:\n",
    "    $$\n",
    "    \\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) + 2 \\cdot \\mathrm{Cov}(X, Y)\n",
    "    $$\n",
    "\n",
    "7.  **Independence:** If two variables $X$ and $Y$ are **independent**, their covariance is **zero**.\n",
    "    \n",
    "    If $X, Y$ are independent, then $\\mathrm{Cov}(X, Y) = 0$.\n",
    "\n",
    "    > **Important:** The reverse is **not** always true. A covariance of zero only means there is no *linear* relationship. Two variables can have a strong non-linear relationship (like a parabola) and still have zero covariance.\n",
    "\n",
    "---\n",
    "\n",
    "## Covariance vs. Variance vs. Correlation\n",
    "\n",
    "This table helps clarify the differences between these related concepts.\n",
    "\n",
    "| Measure | What It Measures | Units | Range |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Variance** | The spread of a **single variable** around its mean. (How much does it vary?) | $\\mathrm{Units}^2$ | $0$ to $+\\infty$ |\n",
    "| **Covariance** | The **directional relationship** between **two variables**. (Do they move together?) | $\\mathrm{Units\\ of\\ } X \\times \\mathrm{Units\\ of\\ } Y$ | $-\\infty$ to $+\\infty$ |\n",
    "| **Correlation** | The **strength and direction** of the linear relationship between **two variables**. | None (Standardized) | $-1$ to $+1$ |\n",
    "\n",
    "**Key Limitation of Covariance:**\n",
    "The main weakness of covariance is that its value is **not standardized**. A covariance of 100 might be very large for one dataset but tiny for another, depending on the units (e.g., dollars vs. cents). This makes it hard to compare the \"strength\" of relationships.\n",
    "\n",
    "**How Correlation \"Fixes\" This:**\n",
    "**Correlation** is simply the **normalized version of covariance**. You calculate it by dividing the covariance by the standard deviations of both variables.\n",
    "\n",
    "$$\n",
    "\\mathrm{Corr}(X, Y) = \\rho_{XY} = \\frac{\\mathrm{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n",
    "$$\n",
    "\n",
    "This standardization scales the value to a range of **-1 to +1**, allowing you to directly compare the *strength* of the linear relationship between different pairs of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9d0184",
   "metadata": {},
   "source": [
    "# ðŸ§® Steps to Calculate the Covariance Matrix\n",
    "\n",
    "## 1. Define the Data Matrix\n",
    "\n",
    "Suppose you have a data matrix $X$ with shape $n \\times m$, where:\n",
    "* $n$ = number of **observations (samples)**\n",
    "* $m$ = number of **variables (features)**\n",
    "\n",
    "**Example:**\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "2.1 & 8.0 \\\\\n",
    "2.5 & 12.0 \\\\\n",
    "3.6 & 14.0 \\\\\n",
    "4.0 & 10.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, we have:\n",
    "* $n = 4$ samples\n",
    "* $m = 2$ variables (letâ€™s call them $X_1$ and $X_2$)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Compute the Mean of Each Variable\n",
    "\n",
    "First, calculate the mean (average) for each variable (column).\n",
    "\n",
    "**Mean of $X_1$:**\n",
    "$$\n",
    "\\bar{X}_1 = \\frac{2.1 + 2.5 + 3.6 + 4.0}{4} = \\frac{12.2}{4} = 3.05\n",
    "$$\n",
    "\n",
    "**Mean of $X_2$:**\n",
    "$$\n",
    "\\bar{X}_2 = \\frac{8 + 12 + 14 + 10}{4} = \\frac{44}{4} = 11\n",
    "$$\n",
    "\n",
    "This gives us a **mean vector** $\\bar{X}$:\n",
    "$$\n",
    "\\bar{X} = [3.05, 11]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Subtract the Mean (Mean-Centering)\n",
    "\n",
    "Next, create a new matrix by subtracting the corresponding variable's mean from each value. This process is called **mean-centering** the data.\n",
    "\n",
    "$$\n",
    "X_c = X - \\bar{X} =\n",
    "\\begin{bmatrix}\n",
    "2.1 - 3.05 & 8 - 11 \\\\\n",
    "2.5 - 3.05 & 12 - 11 \\\\\n",
    "3.6 - 3.05 & 14 - 11 \\\\\n",
    "4.0 - 3.05 & 10 - 11\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-0.95 & -3 \\\\\n",
    "-0.55 & 1 \\\\\n",
    "0.55 & 3 \\\\\n",
    "0.95 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Compute the Covariance Matrix\n",
    "\n",
    "The formula for the **sample covariance matrix** ($\\Sigma$) is:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{n - 1} (X_c)^T (X_c)\n",
    "$$\n",
    "\n",
    "Where $(X_c)^T$ is the transpose of the centered matrix.\n",
    "\n",
    "**Step 4a: Transpose the centered matrix**\n",
    "\n",
    "$$\n",
    "(X_c)^T =\n",
    "\\begin{bmatrix}\n",
    "-0.95 & -0.55 & 0.55 & 0.95 \\\\\n",
    "-3 & 1 & 3 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 4b: Multiply the transpose by the centered matrix**\n",
    "\n",
    "$$\n",
    "(X_c)^T (X_c) =\n",
    "\\begin{bmatrix}\n",
    "-0.95 & -0.55 & 0.55 & 0.95 \\\\\n",
    "-3 & 1 & 3 & -1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "-0.95 & -3 \\\\\n",
    "-0.55 & 1 \\\\\n",
    "0.55 & 3 \\\\\n",
    "0.95 & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Let's compute the four elements of the resulting matrix:\n",
    "\n",
    "* **Top-left (Var $X_1$):**\n",
    "    $(-0.95)^2 + (-0.55)^2 + (0.55)^2 + (0.95)^2 = 0.9025 + 0.3025 + 0.3025 + 0.9025 = 2.41$\n",
    "* **Bottom-right (Var $X_2$):**\n",
    "    $(-3)^2 + (1)^2 + (3)^2 + (-1)^2 = 9 + 1 + 9 + 1 = 20$\n",
    "* **Top-right (Cov $X_1, X_2$):**\n",
    "    $(-0.95)(-3) + (-0.55)(1) + (0.55)(3) + (0.95)(-1) = 2.85 - 0.55 + 1.65 - 0.95 = 3.0$\n",
    "* **Bottom-left (Cov $X_2, X_1$):**\n",
    "    $(-3)(-0.95) + (1)(-0.55) + (3)(0.55) + (-1)(0.95) = 2.85 - 0.55 + 1.65 - 0.95 = 3.0$\n",
    "\n",
    "This gives the matrix of \"sum of squares\":\n",
    "$$\n",
    "(X_c)^T (X_c) =\n",
    "\\begin{bmatrix}\n",
    "2.41 & 3.0 \\\\\n",
    "3.0 & 20\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 4c: Divide by $n - 1$**\n",
    "\n",
    "We divide by $n - 1 = 4 - 1 = 3$ (this is the degrees of freedom for a *sample* covariance).\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{3}\n",
    "\\begin{bmatrix}\n",
    "2.41 & 3.0 \\\\\n",
    "3.0 & 20\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2.41 / 3 & 3.0 / 3 \\\\\n",
    "3.0 / 3 & 20 / 3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.8033... & 1.0 \\\\\n",
    "1.0 & 6.666...\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Final Covariance Matrix\n",
    "\n",
    "Rounding to four decimal places, the covariance matrix is:\n",
    "\n",
    "$$\n",
    "\\Sigma =\n",
    "\\begin{bmatrix}\n",
    "0.8033 & 1.0 \\\\\n",
    "1.0 & 6.6667\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Interpretation\n",
    "\n",
    "The covariance matrix $\\Sigma = \\begin{bmatrix} \\sigma^2(X_1) & \\text{Cov}(X_1, X_2) \\\\ \\text{Cov}(X_2, X_1) & \\sigma^2(X_2) \\end{bmatrix}$ tells us:\n",
    "\n",
    "* **Diagonal (Variances):**\n",
    "    * **Variance of $X_1$:** $\\sigma^2(X_1) \\approx 0.8033$. This measures the spread of the first variable.\n",
    "    * **Variance of $X_2$:** $\\sigma^2(X_2) \\approx 6.6667$. The second variable is much more spread out than the first.\n",
    "* **Off-Diagonal (Covariance):**\n",
    "    * **Covariance between $X_1$ and $X_2$:** $\\text{Cov}(X_1, X_2) = 1.0$.\n",
    "    * Since the covariance is **positive**, it indicates that as $X_1$ increases, $X_2$ tends to increase as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e36363c",
   "metadata": {},
   "source": [
    "## Covariance Properties and Calculation With Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab887392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Initial Data Setup ---\n",
      "Number of samples (n): 4\n",
      "Number of variables: 3\n",
      "\n",
      "Initial Data Matrix:\n",
      "[[ 2.1  8.   1. ]\n",
      " [ 2.5 12.   3. ]\n",
      " [ 3.6 14.   2. ]\n",
      " [ 4.  10.   4. ]]\n",
      "\n",
      "Variable X1: [2.1 2.5 3.6 4. ]\n",
      "Variable X2: [ 8. 12. 14. 10.]\n",
      "Variable X3: [1. 3. 2. 4.]\n",
      "\n",
      "Constant (a): 5\n",
      "Constant (b): 10\n",
      "\n",
      "Initial Variances:\n",
      "  Var(X1): 0.8033\n",
      "  Var(X2): 6.6667\n",
      "  Var(X3): 1.6667\n",
      "\n",
      "Initial Covariances:\n",
      "  Cov(X1, X2): 1.0000\n",
      "  Cov(X1, X3): 0.7667\n",
      "  Cov(X2, X3): 0.6667\n",
      "========================================\n",
      "\n",
      "### Property 1: Symmetry ###\n",
      "  Testing: Cov(X1, X2) = Cov(X2, X1)\n",
      "  LHS (Cov(X1, X2)): 1.0000\n",
      "  RHS (Cov(X2, X1)): 1.0000\n",
      "  Property holds: True\n",
      "----------------------------------------\n",
      "\n",
      "### Property 2: Covariance with Itself ###\n",
      "  Testing: Cov(X1, X1) = Var(X1)\n",
      "  LHS (Cov(X1, X1)): 0.8033\n",
      "  RHS (Var(X1)):    0.8033\n",
      "  Property holds: True\n",
      "----------------------------------------\n",
      "\n",
      "### Property 3: Effect of Adding a Constant ###\n",
      "  Testing: Cov(X1 + 5, X2) = Cov(X1, X2)\n",
      "  New variable (X1 + 5): [7.1 7.5 8.6 9. ]\n",
      "  LHS (Cov(X1 + 5, X2)): 1.0000\n",
      "  RHS (Cov(X1, X2)):      1.0000\n",
      "  Property holds: True\n",
      "----------------------------------------\n",
      "\n",
      "### Property 4: Effect of Scaling ###\n",
      "  Testing: Cov(5*X1, X2) = 5 * Cov(X1, X2)\n",
      "  New variable (5*X1): [np.float64(10.5), np.float64(12.5), np.float64(18.0), np.float64(20.0)]\n",
      "  LHS (Cov(5*X1, X2)): 5.0000\n",
      "  RHS (5 * Cov(X1, X2)): 5 * 1.0000 = 5.0000\n",
      "  Property holds: True\n",
      "----------------------------------------\n",
      "\n",
      "### Property 5: Bilinearity ###\n",
      "  Testing: Cov(5*X1 + 10*X2, X3) = 5*Cov(X1, X3) + 10*Cov(X2, X3)\n",
      "  New variable (5*X1 + 10*X2): [ 90.5 132.5 158.  120. ]\n",
      "  LHS (Cov(5*X1 + 10*X2, X3)): 10.5000\n",
      "  RHS (5*Cov(X1, X3) + 10*Cov(X2, X3)):\n",
      "    = (5 * 0.7667) + (10 * 0.6667)\n",
      "    = (3.8333) + (6.6667)\n",
      "    = 10.5000\n",
      "  Property holds: True\n",
      "----------------------------------------\n",
      "\n",
      "### Property 6: Covariance of Independent Variables ###\n",
      "  Testing: Cov(X1, X3) â‰ˆ 0 (if constructed to be unrelated)\n",
      "  Cov(X1, X3): 0.7667\n",
      "  Cov(X2, X3): 0.6667\n",
      "  (Note: The covariance values 0.7667 and 0.6667 are not exactly 0,\n",
      "  as this is a small sample. In this specific case, they are not close to 0.)\n",
      "  The original Cov(X1, X2) was 1.0000, showing a clear relationship.\n",
      "----------------------------------------\n",
      "\n",
      "### Property 7: Variance of Sums ###\n",
      "  Testing: Var(X1 + X2) = Var(X1) + Var(X2) + 2*Cov(X1, X2)\n",
      "  New variable (X1 + X2): [10.1 14.5 17.6 14. ]\n",
      "  LHS (Var(X1 + X2)): 9.4700\n",
      "  RHS (Var(X1) + Var(X2) + 2*Cov(X1, X2)):\n",
      "    = 0.8033 + 6.6667 + (2 * 1.0000)\n",
      "    = 0.8033 + 6.6667 + 2.0000\n",
      "    = 9.4700\n",
      "  Property holds: True\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 1. Setup Specific Data ---\n",
    "# Define specific, non-random data\n",
    "# Let's use the matrix from the original example, plus a third variable\n",
    "data = np.array([\n",
    "    [2.1, 8.0, 1.0],\n",
    "    [2.5, 12.0, 3.0],\n",
    "    [3.6, 14.0, 2.0],\n",
    "    [4.0, 10.0, 4.0]\n",
    "])\n",
    "\n",
    "# Extract variables (columns)\n",
    "X1 = data[:, 0]\n",
    "X2 = data[:, 1]\n",
    "X3 = data[:, 2]\n",
    "\n",
    "# Define constants\n",
    "a = 5\n",
    "b = 10\n",
    "\n",
    "# --- Helper Function ---\n",
    "def cov(var1, var2):\n",
    "    \"\"\"Calculates the sample covariance between two variables.\"\"\"\n",
    "    # np.cov expects rows to be variables, so we stack them\n",
    "    # ddof=1 ensures we use the (n-1) sample covariance formula\n",
    "    return np.cov(var1, var2, ddof=1)[0, 1]\n",
    "\n",
    "def var(var1):\n",
    "    \"\"\"Calculates the sample variance.\"\"\"\n",
    "    # ddof=1 ensures we use the (n-1) sample variance formula\n",
    "    return np.var(var1, ddof=1)\n",
    "\n",
    "print(\"--- 1. Initial Data Setup ---\")\n",
    "print(f\"Number of samples (n): {data.shape[0]}\")\n",
    "print(f\"Number of variables: {data.shape[1]}\")\n",
    "print(f\"\\nInitial Data Matrix:\\n{data}\")\n",
    "print(f\"\\nVariable X1: {X1}\")\n",
    "print(f\"Variable X2: {X2}\")\n",
    "print(f\"Variable X3: {X3}\")\n",
    "print(f\"\\nConstant (a): {a}\")\n",
    "print(f\"Constant (b): {b}\")\n",
    "\n",
    "# Calculate initial variances and covariances\n",
    "var_x1 = var(X1)\n",
    "var_x2 = var(X2)\n",
    "var_x3 = var(X3)\n",
    "cov_x1_x2 = cov(X1, X2)\n",
    "cov_x1_x3 = cov(X1, X3)\n",
    "cov_x2_x3 = cov(X2, X3)\n",
    "\n",
    "print(f\"\\nInitial Variances:\")\n",
    "print(f\"  Var(X1): {var_x1:.4f}\")\n",
    "print(f\"  Var(X2): {var_x2:.4f}\")\n",
    "print(f\"  Var(X3): {var_x3:.4f}\")\n",
    "print(f\"\\nInitial Covariances:\")\n",
    "print(f\"  Cov(X1, X2): {cov_x1_x2:.4f}\")\n",
    "print(f\"  Cov(X1, X3): {cov_x1_x3:.4f}\")\n",
    "print(f\"  Cov(X2, X3): {cov_x2_x3:.4f}\")\n",
    "print(\"=\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "# --- 2. Prove Properties (Detailed) ---\n",
    "\n",
    "print(\"### Property 1: Symmetry ###\")\n",
    "print(\"  Testing: Cov(X1, X2) = Cov(X2, X1)\")\n",
    "cov_x2_x1 = cov(X2, X1)\n",
    "print(f\"  LHS (Cov(X1, X2)): {cov_x1_x2:.4f}\")\n",
    "print(f\"  RHS (Cov(X2, X1)): {cov_x2_x1:.4f}\")\n",
    "print(f\"  Property holds: {np.isclose(cov_x1_x2, cov_x2_x1)}\")\n",
    "print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"### Property 2: Covariance with Itself ###\")\n",
    "print(\"  Testing: Cov(X1, X1) = Var(X1)\")\n",
    "cov_x1_x1 = cov(X1, X1)\n",
    "print(f\"  LHS (Cov(X1, X1)): {cov_x1_x1:.4f}\")\n",
    "print(f\"  RHS (Var(X1)):    {var_x1:.4f}\")\n",
    "print(f\"  Property holds: {np.isclose(cov_x1_x1, var_x1)}\")\n",
    "print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"### Property 3: Effect of Adding a Constant ###\")\n",
    "print(f\"  Testing: Cov(X1 + {a}, X2) = Cov(X1, X2)\")\n",
    "print(f\"  New variable (X1 + {a}): {X1 + a}\")\n",
    "cov_x1a_x2 = cov(X1 + a, X2)\n",
    "print(f\"  LHS (Cov(X1 + {a}, X2)): {cov_x1a_x2:.4f}\")\n",
    "print(f\"  RHS (Cov(X1, X2)):      {cov_x1_x2:.4f}\")\n",
    "print(f\"  Property holds: {np.isclose(cov_x1a_x2, cov_x1_x2)}\")\n",
    "print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"### Property 4: Effect of Scaling ###\")\n",
    "print(f\"  Testing: Cov({a}*X1, X2) = {a} * Cov(X1, X2)\")\n",
    "print(f\"  New variable ({a}*X1): {[round(x, 2) for x in (a*X1)]}\")\n",
    "# Left side\n",
    "cov_ax1_x2 = cov(a * X1, X2)\n",
    "print(f\"  LHS (Cov({a}*X1, X2)): {cov_ax1_x2:.4f}\")\n",
    "# Right side\n",
    "a_cov_x1_x2 = a * cov_x1_x2\n",
    "print(f\"  RHS ({a} * Cov(X1, X2)): {a} * {cov_x1_x2:.4f} = {a_cov_x1_x2:.4f}\")\n",
    "print(f\"  Property holds: {np.isclose(cov_ax1_x2, a_cov_x1_x2)}\")\n",
    "print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"### Property 5: Bilinearity ###\")\n",
    "print(f\"  Testing: Cov({a}*X1 + {b}*X2, X3) = {a}*Cov(X1, X3) + {b}*Cov(X2, X3)\")\n",
    "# Left side\n",
    "new_var_ab = a * X1 + b * X2\n",
    "print(f\"  New variable ({a}*X1 + {b}*X2): {new_var_ab}\")\n",
    "left_side = cov(new_var_ab, X3)\n",
    "print(f\"  LHS (Cov({a}*X1 + {b}*X2, X3)): {left_side:.4f}\")\n",
    "# Right side\n",
    "right_side = a * cov_x1_x3 + b * cov_x2_x3\n",
    "print(f\"  RHS ({a}*Cov(X1, X3) + {b}*Cov(X2, X3)):\")\n",
    "print(f\"    = ({a} * {cov_x1_x3:.4f}) + ({b} * {cov_x2_x3:.4f})\")\n",
    "print(f\"    = ({(a * cov_x1_x3):.4f}) + ({(b * cov_x2_x3):.4f})\")\n",
    "print(f\"    = {right_side:.4f}\")\n",
    "print(f\"  Property holds: {np.isclose(left_side, right_side)}\")\n",
    "print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"### Property 6: Covariance of Independent Variables ###\")\n",
    "print(\"  Testing: Cov(X1, X3) â‰ˆ 0 (if constructed to be unrelated)\")\n",
    "print(f\"  Cov(X1, X3): {cov_x1_x3:.4f}\")\n",
    "print(f\"  Cov(X2, X3): {cov_x2_x3:.4f}\")\n",
    "print(f\"  (Note: The covariance values {cov_x1_x3:.4f} and {cov_x2_x3:.4f} are not exactly 0,\")\n",
    "print(f\"  as this is a small sample. In this specific case, they are not close to 0.)\")\n",
    "print(f\"  The original Cov(X1, X2) was {cov_x1_x2:.4f}, showing a clear relationship.\")\n",
    "print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "print(\"### Property 7: Variance of Sums ###\")\n",
    "print(\"  Testing: Var(X1 + X2) = Var(X1) + Var(X2) + 2*Cov(X1, X2)\")\n",
    "# Left side\n",
    "new_var_sum = X1 + X2\n",
    "print(f\"  New variable (X1 + X2): {new_var_sum}\")\n",
    "var_x1_plus_x2 = var(new_var_sum)\n",
    "print(f\"  LHS (Var(X1 + X2)): {var_x1_plus_x2:.4f}\")\n",
    "# Right side\n",
    "right_side = var_x1 + var_x2 + 2 * cov_x1_x2\n",
    "print(f\"  RHS (Var(X1) + Var(X2) + 2*Cov(X1, X2)):\")\n",
    "print(f\"    = {var_x1:.4f} + {var_x2:.4f} + (2 * {cov_x1_x2:.4f})\")\n",
    "print(f\"    = {var_x1:.4f} + {var_x2:.4f} + {(2 * cov_x1_x2):.4f}\")\n",
    "print(f\"    = {right_side:.4f}\")\n",
    "print(f\"  Property holds: {np.isclose(var_x1_plus_x2, right_side)}\")\n",
    "print(\"=\" * 40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2f5f3cc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "--- 1. Initial Data Setup ---\n",
    "Number of samples (n): 4\n",
    "Number of variables: 3\n",
    "\n",
    "Initial Data Matrix:\n",
    "[[ 2.1  8.   1. ]\n",
    " [ 2.5 12.   3. ]\n",
    " [ 3.6 14.   2. ]\n",
    " [ 4.  10.   4. ]]\n",
    "\n",
    "Variable X1: [2.1 2.5 3.6 4. ]\n",
    "Variable X2: [ 8. 12. 14. 10.]\n",
    "Variable X3: [1. 3. 2. 4.]\n",
    "\n",
    "Constant (a): 5\n",
    "Constant (b): 10\n",
    "\n",
    "Initial Variances:\n",
    "  Var(X1): 0.8033\n",
    "  Var(X2): 6.6667\n",
    "  Var(X3): 1.6667\n",
    "\n",
    "Initial Covariances:\n",
    "  Cov(X1, X2): 1.0000\n",
    "  Cov(X1, X3): 0.7667\n",
    "  Cov(X2, X3): 0.6667\n",
    "========================================\n",
    "\n",
    "### Property 1: Symmetry ###\n",
    "  Testing: Cov(X1, X2) = Cov(X2, X1)\n",
    "  LHS (Cov(X1, X2)): 1.0000\n",
    "  RHS (Cov(X2, X1)): 1.0000\n",
    "  Property holds: True\n",
    "----------------------------------------\n",
    "\n",
    "### Property 2: Covariance with Itself ###\n",
    "  Testing: Cov(X1, X1) = Var(X1)\n",
    "  LHS (Cov(X1, X1)): 0.8033\n",
    "  RHS (Var(X1)):    0.8033\n",
    "  Property holds: True\n",
    "----------------------------------------\n",
    "\n",
    "### Property 3: Effect of Adding a Constant ###\n",
    "  Testing: Cov(X1 + 5, X2) = Cov(X1, X2)\n",
    "  New variable (X1 + 5): [7.1 7.5 8.6 9. ]\n",
    "  LHS (Cov(X1 + 5, X2)): 1.0000\n",
    "  RHS (Cov(X1, X2)):      1.0000\n",
    "  Property holds: True\n",
    "----------------------------------------\n",
    "\n",
    "### Property 4: Effect of Scaling ###\n",
    "  Testing: Cov(5*X1, X2) = 5 * Cov(X1, X2)\n",
    "  New variable (5*X1): [10.5, 12.5, 18.0, 20.0]\n",
    "  LHS (Cov(5*X1, X2)): 5.0000\n",
    "  RHS (5 * Cov(X1, X2)): 5 * 1.0000 = 5.0000\n",
    "  Property holds: True\n",
    "----------------------------------------\n",
    "\n",
    "### Property 5: Bilinearity ###\n",
    "  Testing: Cov(5*X1 + 10*X2, X3) = 5*Cov(X1, X3) + 10*Cov(X2, X3)\n",
    "  New variable (5*X1 + 10*X2): [ 90.5 132.5 158.  120. ]\n",
    "  LHS (Cov(5*X1 + 10*X2, X3)): 10.5000\n",
    "  RHS (5*Cov(X1, X3) + 10*Cov(X2, X3)):\n",
    "    = (5 * 0.7667) + (10 * 0.6667)\n",
    "    = (3.8333) + (6.6667)\n",
    "    = 10.5000\n",
    "  Property holds: True\n",
    "----------------------------------------\n",
    "\n",
    "### Property 6: Covariance of Independent Variables ###\n",
    "  Testing: Cov(X1, X3) â‰ˆ 0 (if constructed to be unrelated)\n",
    "  Cov(X1, X3): 0.7667\n",
    "  Cov(X2, X3): 0.6667\n",
    "  (Note: The covariance values 0.7667 and 0.6667 are not exactly 0,\n",
    "  as this is a small sample. In this specific case, they are not close to 0.)\n",
    "  The original Cov(X1, X2) was 1.0000, showing a clear relationship.\n",
    "----------------------------------------\n",
    "\n",
    "### Property 7: Variance of Sums ###\n",
    "  Testing: Var(X1 + X2) = Var(X1) + Var(X2) + 2*Cov(X1, X2)\n",
    "  New variable (X1 + X2): [10.1 14.5 17.6 14. ]\n",
    "  LHS (Var(X1 + X2)): 9.4700\n",
    "  RHS (Var(X1) + Var(X2) + 2*Cov(X1, X2)):\n",
    "    = 0.8033 + 6.6667 + (2 * 1.0000)\n",
    "    = 0.8033 + 6.6667 + 2.0000\n",
    "    = 9.4700\n",
    "  Property holds: True\n",
    "========================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor-geometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
